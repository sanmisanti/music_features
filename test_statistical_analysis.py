"""
Test script for the statistical analysis module

This script tests the descriptive statistics functionality.
"""

import sys
import os
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from exploratory_analysis.data_loading import DataLoader
from exploratory_analysis.statistical_analysis import DescriptiveStats
from exploratory_analysis.statistical_analysis.descriptive_stats import quick_stats, print_summary
import pandas as pd

def test_descriptive_stats():
    """Test descriptive statistics functionality"""
    print("ğŸ“Š Testing Descriptive Statistics Module")
    print("=" * 60)
    
    # Load sample data
    print("ğŸ” Loading sample data...")
    loader = DataLoader()
    result = loader.load_dataset('sample_500', sample_size=200, validate=True)
    
    if not result.success:
        print("âŒ Failed to load data for testing")
        return
    
    df = result.data
    print(f"âœ… Loaded {len(df)} rows for analysis")
    
    # Initialize descriptive stats analyzer
    analyzer = DescriptiveStats()
    
    # Run comprehensive analysis
    print("\nğŸ§® Running comprehensive statistical analysis...")
    analysis_results = analyzer.analyze_dataset(df)
    
    if not analysis_results:
        print("âŒ Failed to run statistical analysis")
        return
    
    # Display dataset-level statistics
    print("\nğŸ“‹ DATASET OVERVIEW:")
    print("-" * 40)
    dataset_stats = analysis_results['dataset_stats']
    print(f"ğŸ“Š Total rows: {dataset_stats.total_rows:,}")
    print(f"ğŸ¼ Features analyzed: {dataset_stats.total_features}")
    print(f"ğŸ’¾ Memory usage: {dataset_stats.memory_usage_mb:.2f} MB")
    print(f"ğŸ“ˆ Missing data: {dataset_stats.missing_data_pct:.2f}%")
    print(f"ğŸ”„ Duplicates: {dataset_stats.duplicate_pct:.2f}%")
    print(f"â­ Overall quality: {dataset_stats.overall_quality.upper()}")
    
    # Feature type breakdown
    print(f"\nğŸµ Features by type:")
    for feat_type, count in dataset_stats.feature_counts.items():
        if count > 0:
            print(f"   {feat_type.capitalize()}: {count}")
    
    # Display correlation preview
    print(f"\nğŸ”— CORRELATION PREVIEW:")
    print("-" * 40)
    corr_preview = analysis_results['correlation_preview']
    
    if 'top_positive' in corr_preview and corr_preview['top_positive']:
        print("ğŸ“ˆ Top positive correlations:")
        for i, corr in enumerate(corr_preview['top_positive'][:3], 1):
            print(f"   {i}. {corr['feature1']} â†” {corr['feature2']}: {corr['correlation']:.3f}")
    
    if 'top_negative' in corr_preview and corr_preview['top_negative']:
        print("ğŸ“‰ Top negative correlations:")
        for i, corr in enumerate(corr_preview['top_negative'][:3], 1):
            print(f"   {i}. {corr['feature1']} â†” {corr['feature2']}: {corr['correlation']:.3f}")
    
    if 'high_correlations' in corr_preview:
        high_corr_count = len(corr_preview['high_correlations'])
        if high_corr_count > 0:
            print(f"âš ï¸  High correlations (>0.7): {high_corr_count}")
    
    # Display distribution summary
    print(f"\nğŸ“Š DISTRIBUTION SUMMARY:")
    print("-" * 40)
    dist_summary = analysis_results['distribution_summary']
    
    summary_items = [
        ('ğŸ“ Approximately normal', 'approximately_normal'),
        ('ğŸ“ Moderately skewed', 'moderately_skewed'), 
        ('ğŸ“ Highly skewed', 'highly_skewed'),
        ('ğŸ¯ Many outliers', 'many_outliers'),
        ('ğŸ“Š Low variance', 'low_variance')
    ]
    
    for label, key in summary_items:
        if key in dist_summary and dist_summary[key]:
            features = dist_summary[key][:3]  # Show first 3
            count = len(dist_summary[key])
            if count <= 3:
                print(f"   {label}: {', '.join(features)}")
            else:
                print(f"   {label}: {', '.join(features)}... ({count} total)")
    
    # Display quality assessment
    print(f"\nâœ… QUALITY ASSESSMENT:")
    print("-" * 40)
    quality = analysis_results['quality_assessment']
    print(f"ğŸ† Overall score: {quality['overall_score']:.1f}/100")
    print(f"ğŸ“ˆ Rating: {quality['quality_rating'].upper()}")
    
    if quality['strengths']:
        print("ğŸ’ª Strengths:")
        for strength in quality['strengths']:
            print(f"   âœ… {strength}")
    
    if quality['issues']:
        print("âš ï¸  Issues:")
        for issue in quality['issues']:
            print(f"   âš ï¸  {issue}")
    
    if quality['recommendations']:
        print("ğŸ’¡ Recommendations:")
        for rec in quality['recommendations']:
            print(f"   ğŸ’¡ {rec}")
    
    # Show feature summary table
    print(f"\nğŸ“‹ FEATURE SUMMARY TABLE:")
    print("-" * 60)
    summary_df = analyzer.get_summary_table()
    
    # Show first few features
    if not summary_df.empty:
        print(summary_df.head(8).to_string(index=False))
        if len(summary_df) > 8:
            print(f"... and {len(summary_df) - 8} more features")
    
    # Test individual feature analysis
    print(f"\nğŸ¯ INDIVIDUAL FEATURE ANALYSIS:")
    print("-" * 40)
    
    test_features = ['energy', 'valence', 'tempo']
    for feature in test_features:
        if feature in analyzer.feature_stats:
            print(f"\nğŸµ {feature.upper()}:")
            feature_summary = analyzer.get_feature_summary(feature)
            if feature_summary:
                basic = feature_summary['basic_stats']
                dist = feature_summary['distribution']
                print(f"   ğŸ“Š Mean: {basic['mean']}, Median: {basic['median']}, Std: {basic['std']}")
                print(f"   ğŸ“ Skewness: {dist['skewness']}, Outliers: {dist['outliers_iqr']}")
                print(f"   ğŸ”¢ Unique values: {dist['unique_values']}")
    
    print(f"\nâœ… Descriptive statistics analysis completed successfully!")
    return True

def test_convenience_functions():
    """Test convenience functions"""
    print(f"\nğŸ› ï¸ Testing Convenience Functions")
    print("=" * 40)
    
    # Load data
    loader = DataLoader()
    result = loader.load_dataset('sample_500', sample_size=50)
    
    if not result.success:
        print("âŒ Failed to load data")
        return
    
    df = result.data
    
    # Test quick_stats
    print("âš¡ Testing quick_stats function...")
    quick_results = quick_stats(df)
    
    if quick_results and 'dataset_stats' in quick_results:
        dataset_stats = quick_results['dataset_stats']
        print(f"âœ… Quick stats: {dataset_stats.total_rows} rows, quality: {dataset_stats.overall_quality}")
    
    # Test print_summary
    print("\nğŸ“„ Testing print_summary function...")
    print_summary(df, features=['energy', 'valence', 'tempo'])
    
    print("âœ… Convenience functions working correctly!")

def main():
    """Main test function"""
    print("ğŸµ STATISTICAL ANALYSIS MODULE TEST")
    print("=" * 70)
    print("Testing the descriptive statistics functionality\n")
    
    try:
        # Run main test
        success = test_descriptive_stats()
        
        if success:
            # Test convenience functions
            test_convenience_functions()
            
            print(f"\nğŸ‰ ALL TESTS PASSED!")
            print(f"ğŸ“Š Statistical analysis module is working correctly")
            print(f"ğŸš€ Ready for integration with visualization module")
        
    except Exception as e:
        print(f"âŒ Test failed with error: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()