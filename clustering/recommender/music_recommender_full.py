#!/usr/bin/env python3
"""
Sistema de Recomendaci√≥n Musical - Dataset Completo (1.2M canciones)
================================================================

Sistema h√≠brido que usa:
- Modelos entrenados con 9,677 canciones representativas
- Recomendaciones desde dataset completo de 1.2M canciones

Uso:
    python music_recommender_full.py --models-dir final_models/method1_pca5_silhouette0314 --song-id "ID" --top-n 10
"""

import pandas as pd
import numpy as np
import argparse
import logging
import json
import joblib
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances
import time

# Caracter√≠sticas musicales esperadas
MUSICAL_FEATURES = [
    'danceability', 'energy', 'key', 'loudness', 'mode',
    'speechiness', 'acousticness', 'instrumentalness', 
    'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'
]

def setup_logging(log_level='INFO'):
    """Configurar sistema de logging"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def parse_arguments():
    """Parsear argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description='Sistema de Recomendaci√≥n Musical - Dataset Completo (1.2M)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Argumentos principales
    parser.add_argument('--models-dir', type=str, required=True,
                       help='Directorio con modelos entrenados (scaler, pca, kmeans)')
    
    parser.add_argument('--full-dataset', type=str, 
                       default='../data/cleaned_data/tracks_features_clean.csv',
                       help='Dataset completo 1.2M canciones (default: tracks_features_clean.csv)')
    
    # Modos de entrada
    parser.add_argument('--song-id', type=str,
                       help='ID de canci√≥n para encontrar similares')
    
    parser.add_argument('--search', type=str,
                       help='Buscar canci√≥n por nombre/artista')
    
    parser.add_argument('--random', action='store_true',
                       help='Seleccionar canci√≥n aleatoria del dataset completo')
    
    parser.add_argument('--interactive', action='store_true',
                       help='Modo interactivo')
    
    # Configuraci√≥n de recomendaci√≥n
    parser.add_argument('--top-n', type=int, default=10,
                       help='N√∫mero de canciones similares (default: 10)')
    
    parser.add_argument('--similarity-metric', choices=['cosine', 'euclidean', 'manhattan'],
                       default='manhattan', help='M√©trica de similitud (default: manhattan)')
    
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help='Nivel de logging (default: INFO)')
    
    # Optimizaci√≥n
    parser.add_argument('--chunk-size', type=int, default=10000,
                       help='Tama√±o de chunk para procesar dataset grande (default: 10000)')
    
    return parser.parse_args()

def load_models_pca(models_dir, logger=None):
    """Cargar modelos PCA entrenados"""
    models_path = Path(models_dir)
    
    if not models_path.exists():
        error_msg = f"‚ùå Directorio de modelos no encontrado: {models_dir}"
        if logger:
            logger.error(error_msg)
        return None, None, None
    
    # Buscar archivos de modelos
    scaler_files = list(models_path.glob('scaler*.pkl'))
    kmeans_files = list(models_path.glob('kmeans*.pkl'))
    pca_files = list(models_path.glob('pca*.pkl'))
    
    if not scaler_files or not kmeans_files or not pca_files:
        error_msg = f"‚ùå Modelos PCA no encontrados en: {models_dir}"
        if logger:
            logger.error(error_msg)
        return None, None, None
    
    try:
        scaler = joblib.load(sorted(scaler_files)[-1])
        kmeans = joblib.load(sorted(kmeans_files)[-1])
        pca = joblib.load(sorted(pca_files)[-1])
        
        if logger:
            logger.info(f"‚úÖ Modelos PCA cargados:")
            logger.info(f"   üîß Scaler: StandardScaler")
            logger.info(f"   üéØ PCA: {pca.n_components} componentes ({pca.explained_variance_ratio_.sum():.1%} varianza)")
            logger.info(f"   üéµ K-Means: {kmeans.n_clusters} clusters")
        
        return scaler, pca, kmeans
        
    except Exception as e:
        error_msg = f"‚ùå Error al cargar modelos: {e}"
        if logger:
            logger.error(error_msg)
        return None, None, None

def load_full_dataset(dataset_path, logger=None):
    """Cargar dataset completo de 1.2M canciones de forma eficiente"""
    if not Path(dataset_path).exists():
        error_msg = f"‚ùå Dataset no encontrado: {dataset_path}"
        if logger:
            logger.error(error_msg)
        return None
    
    if logger:
        logger.info(f"üîç Cargando dataset completo: {dataset_path}")
    
    start_time = time.time()
    
    try:
        # Cargar con optimizaciones para dataset grande
        df = pd.read_csv(
            dataset_path, 
            sep=';', 
            decimal=',', 
            encoding='utf-8', 
            on_bad_lines='skip',
            low_memory=False,
            dtype={
                'year': 'Int64',  # Permite NaN
                'explicit': 'bool'
            }
        )
        
        load_time = time.time() - start_time
        memory_usage = df.memory_usage(deep=True).sum() / 1024**2  # MB
        
        if logger:
            logger.info(f"‚úÖ Dataset completo cargado:")
            logger.info(f"   üéµ Canciones: {len(df):,}")
            logger.info(f"   ‚è±Ô∏è  Tiempo: {load_time:.2f}s")
            logger.info(f"   üíæ Memoria: {memory_usage:.1f}MB")
        
        return df
        
    except Exception as e:
        error_msg = f"‚ùå Error al cargar dataset: {e}"
        if logger:
            logger.error(error_msg)
        return None

def vectorize_song(song_features, scaler, pca):
    """Vectorizar una canci√≥n usando el pipeline PCA"""
    # Convertir a array si es diccionario
    if isinstance(song_features, dict):
        features_array = np.array([[song_features[col] for col in MUSICAL_FEATURES]])
    else:
        features_array = song_features.reshape(1, -1)
    
    # Pipeline: Normalizar ‚Üí PCA
    normalized = scaler.transform(features_array)
    vectorized = pca.transform(normalized)
    
    return vectorized

def predict_cluster(song_vector, kmeans):
    """Predecir cluster de una canci√≥n vectorizada"""
    cluster = kmeans.predict(song_vector)[0]
    distances = kmeans.transform(song_vector)[0]
    return cluster, distances

def find_similar_songs_optimized(target_song, full_dataset, scaler, pca, kmeans, 
                                similarity_metric='manhattan', top_n=10, 
                                chunk_size=10000, logger=None):
    """
    Encontrar canciones similares de forma optimizada para dataset grande
    """
    if logger:
        logger.info(f"üéØ Buscando canciones similares en {len(full_dataset):,} canciones...")
    
    start_time = time.time()
    
    # 1. Vectorizar canci√≥n objetivo
    target_features = {col: target_song[col] for col in MUSICAL_FEATURES}
    target_vector = vectorize_song(target_features, scaler, pca)
    target_cluster, cluster_distances = predict_cluster(target_vector, kmeans)
    
    if logger:
        logger.info(f"   üéØ Cluster predicho: {target_cluster}")
        logger.debug(f"   üìä Distancias: {cluster_distances}")
    
    # 2. Procesar dataset por chunks para eficiencia
    all_similarities = []
    total_processed = 0
    
    for chunk_start in range(0, len(full_dataset), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(full_dataset))
        chunk = full_dataset.iloc[chunk_start:chunk_end].copy()
        
        # Extraer features del chunk
        chunk_features = chunk[MUSICAL_FEATURES].values
        
        # Vectorizar chunk completo
        chunk_normalized = scaler.transform(chunk_features)
        chunk_vectorized = pca.transform(chunk_normalized)
        
        # Predecir clusters del chunk
        chunk_clusters = kmeans.predict(chunk_vectorized)
        
        # Filtrar solo canciones del mismo cluster
        same_cluster_mask = (chunk_clusters == target_cluster)
        same_cluster_indices = np.where(same_cluster_mask)[0]
        
        if len(same_cluster_indices) > 0:
            # Calcular similitudes solo con canciones del mismo cluster
            cluster_vectors = chunk_vectorized[same_cluster_indices]
            
            if similarity_metric == 'cosine':
                similarities = cosine_similarity(target_vector, cluster_vectors)[0]
            elif similarity_metric == 'euclidean':
                distances = euclidean_distances(target_vector, cluster_vectors)[0]
                similarities = 1 / (1 + distances)
            else:  # manhattan
                distances = manhattan_distances(target_vector, cluster_vectors)[0]
                similarities = 1 / (1 + distances)
            
            # Crear DataFrame con resultados del chunk
            chunk_results = chunk.iloc[same_cluster_indices].copy()
            chunk_results['similarity'] = similarities
            chunk_results['chunk_index'] = chunk_start + same_cluster_indices
            
            all_similarities.append(chunk_results)
        
        total_processed += len(chunk)
        
        if logger and total_processed % (chunk_size * 10) == 0:
            logger.debug(f"   üìä Procesadas: {total_processed:,}/{len(full_dataset):,} canciones")
    
    # 3. Combinar resultados y encontrar top-N
    if not all_similarities:
        if logger:
            logger.warning("‚ö†Ô∏è  No se encontraron canciones en el mismo cluster")
        return pd.DataFrame(), target_cluster, cluster_distances
    
    # Concatenar todos los resultados
    combined_results = pd.concat(all_similarities, ignore_index=True)
    
    # Excluir la canci√≥n original si est√° presente
    if 'id' in target_song:
        combined_results = combined_results[combined_results['id'] != target_song['id']]
    
    # Obtener top-N m√°s similares
    top_similar = combined_results.nlargest(top_n, 'similarity')
    
    processing_time = time.time() - start_time
    
    if logger:
        logger.info(f"‚úÖ B√∫squeda completada:")
        logger.info(f"   üéµ Candidatos en cluster {target_cluster}: {len(combined_results):,}")
        logger.info(f"   üèÜ Top similares: {len(top_similar)}")
        logger.info(f"   ‚è±Ô∏è  Tiempo: {processing_time:.2f}s")
    
    return top_similar, target_cluster, cluster_distances

def print_recommendations_full(target_song, similar_songs, cluster, cluster_distances, 
                              similarity_metric, dataset_size):
    """Mostrar recomendaciones de forma formateada"""
    
    print("\n" + "="*80)
    print("üéµ RECOMENDACIONES MUSICALES - DATASET COMPLETO")
    print("="*80)
    
    print(f"üìÄ Canci√≥n de referencia:")
    print(f"   üé§ \"{target_song['name']}\" - {target_song['artists']}")
    if 'year' in target_song and pd.notna(target_song['year']):
        print(f"   üìÖ A√±o: {int(target_song['year'])}")
    if 'album' in target_song:
        print(f"   üíø √Ålbum: {target_song['album']}")
    
    print(f"\nüéØ Informaci√≥n del clustering:")
    print(f"   üìä Dataset: {dataset_size:,} canciones")
    print(f"   üéØ Cluster asignado: {cluster}")
    print(f"   üìà Distancias a centroides: {[f'{d:.3f}' for d in cluster_distances]}")
    print(f"   üìä M√©trica de similitud: {similarity_metric}")
    
    if similar_songs.empty:
        print("\n‚ùå No se encontraron canciones similares")
        return
    
    print(f"\nüéµ Top {len(similar_songs)} canciones m√°s similares:")
    print("-" * 80)
    
    for i, (_, song) in enumerate(similar_songs.iterrows(), 1):
        similarity_pct = song['similarity'] * 100
        year_str = f"{int(song['year'])}" if 'year' in song and pd.notna(song['year']) else 'N/A'
        
        print(f"{i:2d}. üé§ \"{song['name']}\" - {song['artists']}")
        print(f"    üìä Similitud: {similarity_pct:.1f}% | üìÖ A√±o: {year_str}")
        if 'album' in song and pd.notna(song['album']):
            print(f"    üíø √Ålbum: {song['album']}")
        if i < len(similar_songs):
            print()

def search_song_in_dataset(search_term, dataset, logger=None):
    """Buscar canci√≥n en el dataset por nombre o artista"""
    if logger:
        logger.info(f"üîç Buscando: '{search_term}'")
    
    search_lower = search_term.lower()
    
    # Buscar en nombre y artistas
    name_matches = dataset[dataset['name'].str.lower().str.contains(search_lower, na=False)]
    artist_matches = dataset[dataset['artists'].str.lower().str.contains(search_lower, na=False)]
    
    # Combinar y eliminar duplicados
    matches = pd.concat([name_matches, artist_matches]).drop_duplicates()
    
    if logger:
        logger.info(f"   üìä Encontradas: {len(matches)} canciones")
    
    return matches

def main():
    """Funci√≥n principal"""
    args = parse_arguments()
    logger = setup_logging(args.log_level)
    
    logger.info("üéµ SISTEMA DE RECOMENDACI√ìN - DATASET COMPLETO (1.2M)")
    logger.info("=" * 80)
    
    try:
        # 1. Cargar modelos PCA
        scaler, pca, kmeans = load_models_pca(args.models_dir, logger)
        if scaler is None:
            return 1
        
        # 2. Cargar dataset completo
        full_dataset = load_full_dataset(args.full_dataset, logger)
        if full_dataset is None:
            return 1
        
        # 3. Procesar seg√∫n modo de entrada
        target_song = None
        
        if args.song_id:
            # Buscar por ID espec√≠fico
            song_matches = full_dataset[full_dataset['id'] == args.song_id]
            if song_matches.empty:
                logger.error(f"‚ùå Canci√≥n no encontrada: {args.song_id}")
                return 1
            target_song = song_matches.iloc[0]
            
        elif args.search:
            # Buscar por t√©rmino
            matches = search_song_in_dataset(args.search, full_dataset, logger)
            if matches.empty:
                logger.error(f"‚ùå No se encontraron canciones con: '{args.search}'")
                return 1
            
            # Mostrar opciones si hay m√∫ltiples matches
            if len(matches) > 1:
                print(f"\nüîç Encontradas {len(matches)} canciones:")
                for i, (_, song) in enumerate(matches.head(10).iterrows(), 1):
                    year_str = f"({int(song['year'])})" if 'year' in song and pd.notna(song['year']) else ""
                    print(f"  {i}. {song['id']} - \"{song['name']}\" - {song['artists']} {year_str}")
                
                if len(matches) > 10:
                    print(f"    ... y {len(matches) - 10} m√°s")
                
                print("\nüí° Usa --song-id con el ID espec√≠fico para obtener recomendaciones")
                return 0
            else:
                target_song = matches.iloc[0]
                
        elif args.random:
            # Seleccionar canci√≥n aleatoria
            target_song = full_dataset.sample(n=1, random_state=42).iloc[0]
            logger.info(f"üé≤ Canci√≥n aleatoria: {target_song['id']}")
            
        else:
            logger.error("‚ùå Debe especificar --song-id, --search, o --random")
            return 1
        
        # 4. Encontrar canciones similares
        if target_song is not None:
            logger.info(f"üéµ Procesando: \"{target_song['name']}\" - {target_song['artists']}")
            
            similar_songs, cluster, cluster_distances = find_similar_songs_optimized(
                target_song, full_dataset, scaler, pca, kmeans,
                args.similarity_metric, args.top_n, args.chunk_size, logger
            )
            
            # 5. Mostrar resultados
            print_recommendations_full(
                target_song, similar_songs, cluster, cluster_distances,
                args.similarity_metric, len(full_dataset)
            )
        
        logger.info("üéµ ¬°RECOMENDACI√ìN COMPLETADA EXITOSAMENTE! üéâ")
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Error durante la ejecuci√≥n: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)