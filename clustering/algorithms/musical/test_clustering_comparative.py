#!/usr/bin/env python3
"""
FASE 2.1.2: TEST INICIAL CLUSTERING COMPARATIVO
===============================================

Script de validaci√≥n para verificar configuraci√≥n de datasets y 
funcionamiento b√°sico del sistema comparativo.

Objetivo: Validar que todos los datasets cargan correctamente antes
de ejecutar an√°lisis completo.
"""

import os
import sys
from pathlib import Path

# A√±adir path para imports - ir al directorio ra√≠z del proyecto
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent.parent
sys.path.insert(0, str(project_root))

# Import directo del comparador en el mismo directorio
from clustering_comparative import ClusteringComparator

def test_dataset_loading():
    """Test carga de todos los datasets configurados."""
    
    print("üß™ FASE 2.1.2: TEST DATASET LOADING")
    print("="*50)
    
    # Inicializar comparador
    comparator = ClusteringComparator()
    
    # Test cada dataset
    datasets_to_test = ['optimal', 'control', 'baseline']
    loading_results = {}
    
    for dataset_key in datasets_to_test:
        print(f"\nüìä Testing dataset: {dataset_key}")
        print("-"*30)
        
        try:
            # Cargar con sample peque√±o para test r√°pido
            features_df, metadata = comparator.load_dataset(
                dataset_key, 
                sample_size=100, 
                test_mode=True
            )
            
            loading_results[dataset_key] = {
                'status': 'success',
                'shape': features_df.shape,
                'features': metadata['n_features'],
                'expected_hopkins': metadata['expected_hopkins'],
                'separator': comparator.datasets_config[dataset_key]['separator']
            }
            
            print(f"‚úÖ {dataset_key}: {features_df.shape[0]} samples √ó {features_df.shape[1]} features")
            print(f"   Expected Hopkins: {metadata['expected_hopkins']}")
            
        except Exception as e:
            loading_results[dataset_key] = {
                'status': 'error',
                'error': str(e)
            }
            print(f"‚ùå {dataset_key}: ERROR - {e}")
    
    # Resumen
    print(f"\nüìä LOADING TEST SUMMARY")
    print("="*30)
    
    success_count = sum(1 for result in loading_results.values() if result['status'] == 'success')
    total_count = len(loading_results)
    
    print(f"‚úÖ Successful loads: {success_count}/{total_count}")
    
    if success_count == total_count:
        print("üéâ ALL DATASETS LOADED SUCCESSFULLY - Ready for clustering comparison")
        return True
    else:
        print("‚ö†Ô∏è  Some datasets failed to load - Check configuration")
        return False

def test_clustering_analysis_quick():
    """Test r√°pido de an√°lisis clustering en dataset optimal."""
    
    print(f"\nüß™ FASE 2.1.3: TEST CLUSTERING ANALYSIS")
    print("="*50)
    
    try:
        # Inicializar comparador
        comparator = ClusteringComparator()
        
        # Cargar dataset optimal (m√°s peque√±o y confiable)
        print("üìÇ Cargando dataset optimal para test...")
        features_df, metadata = comparator.load_dataset('optimal', sample_size=200, test_mode=True)
        
        # Ejecutar an√°lisis clustering reducido
        print("üéØ Ejecutando an√°lisis clustering test...")
        
        # Configuraci√≥n reducida para test r√°pido
        original_config = comparator.analysis_config.copy()
        comparator.analysis_config.update({
            'k_range': [3, 5, 7],  # Solo 3 valores K
            'n_runs': 3,           # Solo 3 runs por K
            'random_states': [42, 43, 44]
        })
        
        analysis_result = comparator.run_clustering_analysis(
            features_df, metadata, algorithm='kmeans'
        )
        
        # Restaurar configuraci√≥n original
        comparator.analysis_config = original_config
        
        # Verificar resultados
        if analysis_result and 'best_k' in analysis_result and analysis_result['best_k']:
            best_k = analysis_result['best_k']
            print(f"‚úÖ Clustering test successful:")
            print(f"   Best K: {best_k['k']}")
            print(f"   Best Silhouette: {best_k['silhouette_score']:.4f}")
            
            # Verificar si cumple objetivo
            target_met = best_k['silhouette_score'] > 0.25
            target_status = "‚úÖ MET" if target_met else "‚ö†Ô∏è  NOT MET"
            print(f"   Target >0.25: {target_status}")
            
            return True
        else:
            print("‚ùå Clustering test failed - No best K found")
            return False
            
    except Exception as e:
        print(f"‚ùå Clustering test failed: {e}")
        return False

def test_comparison_quick():
    """Test r√°pido de comparaci√≥n entre datasets."""
    
    print(f"\nüß™ FASE 2.1.4: TEST COMPARISON SYSTEM")
    print("="*50)
    
    try:
        # Inicializar comparador
        comparator = ClusteringComparator()
        
        # Ejecutar comparaci√≥n reducida (solo optimal vs control)
        print("üìä Ejecutando comparaci√≥n test...")
        
        comparison_results = comparator.compare_datasets(
            dataset_keys=['optimal', 'control'],  # Solo 2 datasets
            algorithm='kmeans',
            test_mode=True
        )
        
        # Verificar resultados
        if comparison_results and 'recommendations' in comparison_results:
            recommendations = comparison_results['recommendations']
            
            print(f"‚úÖ Comparison test successful:")
            
            if recommendations.get('best_dataset'):
                best_info = recommendations['best_dataset']
                print(f"   Best dataset: {best_info['dataset']}")
                print(f"   Best silhouette: {best_info['silhouette_score']:.4f}")
                
                target_met = recommendations.get('silhouette_target_met', False)
                target_status = "‚úÖ YES" if target_met else "‚ùå NO"
                print(f"   Target >0.25 met: {target_status}")
                
                significance = recommendations.get('statistical_significance', False)
                sig_status = "‚úÖ YES" if significance else "‚ùå NO" 
                print(f"   Statistical significance: {sig_status}")
            
            # Guardar resultados test
            json_path, report_path = comparator.save_results(comparison_results, "_validation_test")
            print(f"üìÅ Test results saved: {json_path.name}")
            
            return True
        else:
            print("‚ùå Comparison test failed - No recommendations generated")
            return False
            
    except Exception as e:
        print(f"‚ùå Comparison test failed: {e}")
        return False

def main():
    """Ejecutar todos los tests de validaci√≥n."""
    
    print("üöÄ CLUSTERING COMPARATIVE VALIDATION TESTS")
    print("="*60)
    print("Objective: Validate system readiness for FASE 2 execution")
    print()
    
    # Ejecutar tests secuencialmente
    tests_results = []
    
    # Test 1: Dataset loading
    test1_success = test_dataset_loading()
    tests_results.append(('Dataset Loading', test1_success))
    
    if test1_success:
        # Test 2: Clustering analysis
        test2_success = test_clustering_analysis_quick()
        tests_results.append(('Clustering Analysis', test2_success))
        
        if test2_success:
            # Test 3: Comparison system
            test3_success = test_comparison_quick()
            tests_results.append(('Comparison System', test3_success))
    
    # Resumen final
    print(f"\nüéØ VALIDATION TESTS SUMMARY")
    print("="*40)
    
    for test_name, success in tests_results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{test_name}: {status}")
    
    total_tests = len(tests_results)
    passed_tests = sum(1 for _, success in tests_results if success)
    
    print(f"\nPassed: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("üéâ ALL TESTS PASSED - SYSTEM READY FOR FASE 2 EXECUTION")
        print("\nNext steps:")
        print("1. Run full comparison: python clustering_comparative.py")
        print("2. Analyze results and proceed to FASE 2.2")
    else:
        print("‚ö†Ô∏è  SOME TESTS FAILED - FIX ISSUES BEFORE PROCEEDING")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    exit_code = 0 if success else 1
    sys.exit(exit_code)