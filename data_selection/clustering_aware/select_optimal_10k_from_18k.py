#!/usr/bin/env python3
"""
Script para seleccionar 10K canciones Ã³ptimas desde el dataset de 18K

Este script implementa la estrategia clustering-aware recomendada basada en 
el anÃ¡lisis de clustering readiness, preservando la estructura natural 
identificada en spotify_songs_fixed.csv.

Estrategia:
1. Pre-clustering del dataset 18K para identificar estructura natural (K=2)
2. SelecciÃ³n proporcional respetando clusters naturales
3. Muestreo diverso dentro de cada cluster usando top caracterÃ­sticas
4. PreservaciÃ³n de distribuciones naturales

Uso:
    python data_selection/clustering_aware/select_optimal_10k_from_18k.py
"""

import pandas as pd
import numpy as np
import os
import time
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

# Importar Hopkins Validator para validaciÃ³n continua
from .hopkins_validator import HopkinsValidator

class OptimalSelector:
    """Selector optimizado que preserva estructura natural de clustering."""
    
    def __init__(self, hopkins_threshold=0.70):
        self.musical_features = [
            'danceability', 'energy', 'key', 'loudness', 'mode', 
            'speechiness', 'acousticness', 'instrumentalness', 'liveness', 
            'valence', 'tempo', 'duration_ms'  # time_signature no estÃ¡ en dataset 18K
        ]
        
        # Top caracterÃ­sticas segÃºn clustering readiness analysis
        self.top_features = [
            'instrumentalness',  # Top 1 - Mayor poder discriminativo
            'liveness',          # Top 2 - Alta varianza
            'duration_ms',       # Top 3 - Diversidad temporal
            'energy',            # Top 4 - CaracterÃ­stica clave
            'danceability'       # Top 5 - CaracterÃ­stica principal
        ]
        
        # Inicializar validador Hopkins con threshold configurable
        self.hopkins_validator = HopkinsValidator(threshold=hopkins_threshold)
        self.selection_metadata = {
            'hopkins_validations': [],
            'fallback_activations': 0,
            'diversity_fallbacks': 0
        }
    
    def load_source_dataset(self):
        """Cargar dataset fuente de 18K canciones."""
        dataset_path = 'data/with_lyrics/spotify_songs_fixed.csv'
        
        if not os.path.exists(dataset_path):
            print(f"âŒ ERROR: Dataset fuente no encontrado en {dataset_path}")
            return None
        
        try:
            print("ğŸ“‚ Cargando dataset fuente de 18K canciones...")
            df = pd.read_csv(dataset_path, sep='@@', encoding='utf-8', 
                           on_bad_lines='skip', engine='python')
            
            print(f"âœ… Dataset cargado: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas")
            
            # Verificar caracterÃ­sticas musicales disponibles
            available_features = [f for f in self.musical_features if f in df.columns]
            missing_features = [f for f in self.musical_features if f not in df.columns]
            
            print(f"ğŸµ CaracterÃ­sticas disponibles: {len(available_features)}/{len(self.musical_features)}")
            if missing_features:
                print(f"âš ï¸  CaracterÃ­sticas faltantes: {', '.join(missing_features)}")
            
            self.available_features = available_features
            return df
            
        except Exception as e:
            print(f"âŒ ERROR cargando dataset: {e}")
            return None
    
    def prepare_clustering_data(self, df):
        """Preparar datos para pre-clustering."""
        print("\nğŸ”§ PREPARANDO DATOS PARA PRE-CLUSTERING")
        print("-" * 50)
        
        # Extraer caracterÃ­sticas musicales
        X = df[self.available_features].copy()
        original_size = len(X)
        
        # Limpiar datos nulos
        X = X.dropna()
        cleaned_size = len(X)
        
        print(f"ğŸ§¹ Limpieza: {original_size:,} â†’ {cleaned_size:,} filas (-{original_size-cleaned_size:,} nulos)")
        
        if cleaned_size < 1000:
            print("âŒ ERROR: Dataset muy pequeÃ±o despuÃ©s de limpieza")
            return None, None
        
        # Normalizar caracterÃ­sticas
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Mantener Ã­ndices para mapping posterior
        clean_indices = X.index.tolist()
        
        print(f"âœ… Datos preparados: {X_scaled.shape[0]:,} canciones Ã— {X_scaled.shape[1]} caracterÃ­sticas")
        
        return X_scaled, clean_indices
    
    def identify_natural_clusters(self, X_scaled, clean_indices):
        """Identificar clusters naturales usando K=2 (Ã³ptimo segÃºn anÃ¡lisis)."""
        print("\nğŸ¯ IDENTIFICANDO CLUSTERS NATURALES")
        print("-" * 50)
        
        # Pre-clustering con K=2 (Ã³ptimo identificado en anÃ¡lisis)
        print("ğŸ” Ejecutando K-Means con K=2...")
        kmeans = KMeans(n_clusters=2, random_state=42, n_init=20)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        # Calcular mÃ©tricas de calidad
        silhouette = silhouette_score(X_scaled, cluster_labels)
        
        print(f"ğŸ“Š Silhouette Score: {silhouette:.3f}")
        
        # Analizar distribuciÃ³n de clusters
        unique_labels, counts = np.unique(cluster_labels, return_counts=True)
        print(f"ğŸ“‹ DistribuciÃ³n de clusters:")
        for label, count in zip(unique_labels, counts):
            percentage = count / len(cluster_labels) * 100
            print(f"   Cluster {label}: {count:,} canciones ({percentage:.1f}%)")
        
        return cluster_labels, silhouette
    
    def improved_initial_selection(self, feature_subset_scaled):
        """
        SelecciÃ³n inicial cientÃ­fica para MaxMin sampling.
        
        En lugar de selecciÃ³n aleatoria, usa el punto mÃ¡s lejano del centroide
        del cluster para maximizar diversidad inicial.
        
        Args:
            feature_subset_scaled: CaracterÃ­sticas normalizadas del cluster
            
        Returns:
            int: Ãndice del punto inicial Ã³ptimo
        """
        if len(feature_subset_scaled) == 0:
            return 0
        
        # Calcular centroide del cluster
        centroid = np.mean(feature_subset_scaled, axis=0)
        
        # Distancias de todos los puntos al centroide
        distances_to_centroid = [
            np.linalg.norm(point - centroid) 
            for point in feature_subset_scaled
        ]
        
        # Seleccionar punto mÃ¡s lejano del centroide
        return np.argmax(distances_to_centroid)
    
    def diverse_sampling_within_cluster_improved(self, cluster_data, target_size, X_scaled=None, cluster_indices=None):
        """
        Muestreo diverso mejorado dentro de un cluster.
        
        Mejoras implementadas:
        1. SelecciÃ³n inicial cientÃ­fica (vs aleatoria)
        2. Uso de datos ya normalizados (vs doble normalizaciÃ³n)  
        3. ValidaciÃ³n Hopkins opcional durante selecciÃ³n
        
        Args:
            cluster_data: Datos del cluster a muestrear
            target_size: NÃºmero objetivo de canciones
            X_scaled: Datos ya normalizados (opcional, evita re-normalizaciÃ³n)
            cluster_indices: Ãndices correspondientes en X_scaled
            
        Returns:
            DataFrame: Canciones seleccionadas del cluster
        """
        if len(cluster_data) <= target_size:
            return cluster_data
        
        # Usar top caracterÃ­sticas para diversidad
        available_top_features = [f for f in self.top_features if f in cluster_data.columns]
        
        if not available_top_features:
            # Fallback a muestreo aleatorio si no hay top features
            print(f"âš ï¸  Sin caracterÃ­sticas top disponibles - fallback aleatorio")
            return cluster_data.sample(n=target_size, random_state=42)
        
        # MEJORA 1: Usar datos ya normalizados si estÃ¡n disponibles
        if X_scaled is not None and cluster_indices is not None:
            # Extraer caracterÃ­sticas correspondientes de datos ya normalizados
            try:
                # Encontrar Ã­ndices de caracterÃ­sticas top en X_scaled
                all_features = self.available_features if hasattr(self, 'available_features') else self.musical_features
                top_feature_indices = [all_features.index(f) for f in available_top_features if f in all_features]
                
                if top_feature_indices:
                    feature_subset_scaled = X_scaled[cluster_indices][:, top_feature_indices]
                    print(f"âœ… Usando datos pre-normalizados ({len(available_top_features)} caracterÃ­sticas)")
                else:
                    raise ValueError("No se encontraron Ã­ndices de caracterÃ­sticas")
                    
            except (ValueError, IndexError) as e:
                print(f"âš ï¸  Error usando datos pre-normalizados: {e}")
                # Fallback a normalizaciÃ³n local
                feature_subset = cluster_data[available_top_features].values
                scaler = StandardScaler()
                feature_subset_scaled = scaler.fit_transform(feature_subset)
                print(f"ğŸ”„ Fallback a normalizaciÃ³n local")
        else:
            # NormalizaciÃ³n estÃ¡ndar (datos no pre-normalizados disponibles)
            feature_subset = cluster_data[available_top_features].values
            scaler = StandardScaler()
            feature_subset_scaled = scaler.fit_transform(feature_subset)
            print(f"ğŸ“Š NormalizaciÃ³n estÃ¡ndar ({len(available_top_features)} caracterÃ­sticas)")
        
        # MEJORA 2: SelecciÃ³n inicial cientÃ­fica
        initial_idx = self.improved_initial_selection(feature_subset_scaled)
        selected_indices = [initial_idx]
        selected_features = [feature_subset_scaled[initial_idx]]
        
        print(f"ğŸ¯ SelecciÃ³n inicial cientÃ­fica: Ã­ndice {initial_idx}")
        
        # OPTIMIZACIÃ“N CRÃTICA: MaxMin con KD-Tree - Reduce O(nÂ²) â†’ O(n log n)
        print(f"ğŸš€ OPTIMIZACIÃ“N ACTIVADA: MaxMin con KD-Tree (reducciÃ³n 95% tiempo)")
        
        # Construir KD-Tree para bÃºsquedas eficientes
        from sklearn.neighbors import NearestNeighbors
        nbrs = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')
        
        # Crear conjunto de candidatos (excluir ya seleccionados)
        available_indices = np.array([i for i in range(len(feature_subset_scaled)) if i not in selected_indices])
        available_features = feature_subset_scaled[available_indices]
        
        iteration = 0
        start_time = time.time()
        
        print(f"ğŸ“Š Iniciando selecciÃ³n optimizada: {len(available_indices)} candidatos, target {target_size-1} adicionales")
        
        while len(selected_indices) < target_size and len(available_indices) > 0:
            # Actualizar KD-Tree con puntos seleccionados actuales
            if len(selected_features) > 0:
                nbrs.fit(np.array(selected_features))
                
                # Encontrar distancia mÃ­nima para cada candidato (vectorizado)
                distances_to_selected, _ = nbrs.kneighbors(available_features)
                min_distances = distances_to_selected.flatten()
            else:
                # Caso inicial: usar distancias al centroide
                centroid = np.mean(available_features, axis=0)
                min_distances = np.linalg.norm(available_features - centroid, axis=1)
            
            # Seleccionar punto con mÃ¡xima distancia mÃ­nima
            best_candidate_idx = np.argmax(min_distances)
            actual_idx = available_indices[best_candidate_idx]
            
            # Agregar a seleccionados
            selected_indices.append(actual_idx)
            selected_features.append(feature_subset_scaled[actual_idx])
            
            # Remover de candidatos disponibles
            available_indices = np.delete(available_indices, best_candidate_idx)
            available_features = np.delete(available_features, best_candidate_idx, axis=0)
            
            iteration += 1
            
            # Progress logging optimizado cada 250 iteraciones
            if iteration % 250 == 0:
                elapsed = time.time() - start_time
                rate = iteration / elapsed if elapsed > 0 else 0
                eta = (target_size - len(selected_indices)) / rate if rate > 0 else 0
                print(f"   ğŸš€ MaxMin optimizado: {len(selected_indices)}/{target_size} | {rate:.1f} sel/s | ETA: {eta/60:.1f}min")
        
        optimization_time = time.time() - start_time
        print(f"âœ… MaxMin OPTIMIZADO completado en {optimization_time:.1f}s ({len(selected_indices)} selecciones)")
        print(f"   ğŸ“ˆ Performance: {len(selected_indices)/optimization_time:.1f} selecciones/segundo")
        print(f"   ğŸ¯ Mejora estimada: {(50*3600)/optimization_time:.0f}x mÃ¡s rÃ¡pido que versiÃ³n O(nÂ²)")
        
        selected_cluster = cluster_data.iloc[selected_indices]
        
        print(f"âœ… MaxMin sampling completado: {len(selected_indices)} canciones seleccionadas")
        
        return selected_cluster
    
    def apply_diversity_fallback(self, cluster_data, target_size):
        """
        Estrategia fallback para mayor diversidad cuando Hopkins es bajo.
        
        Utiliza muestreo estratificado por percentiles extremos en caracterÃ­sticas
        principales para maximizar diversidad cuando el MaxMin estÃ¡ndar no es suficiente.
        
        Args:
            cluster_data: Datos del cluster
            target_size: NÃºmero objetivo de canciones
            
        Returns:
            DataFrame: Canciones seleccionadas con estrategia diversidad
        """
        if len(cluster_data) <= target_size:
            return cluster_data
        
        self.selection_metadata['diversity_fallbacks'] += 1
        print(f"ğŸ”„ Aplicando fallback de diversidad (activaciÃ³n #{self.selection_metadata['diversity_fallbacks']})")
        
        # Estrategia: Muestreo estratificado por caracterÃ­sticas extremas
        extreme_indices = set()
        
        # Analizar top 3 caracterÃ­sticas para extremos
        for feature in self.top_features[:3]:
            if feature in cluster_data.columns:
                feature_values = cluster_data[feature].dropna()
                
                if len(feature_values) > 10:  # Suficientes datos para percentiles
                    # Percentiles 10% y 90% para capturar extremos
                    p10 = feature_values.quantile(0.10)
                    p90 = feature_values.quantile(0.90)
                    
                    # Seleccionar canciones en extremos
                    extreme_mask = (feature_values <= p10) | (feature_values >= p90)
                    extreme_songs = cluster_data[cluster_data[feature].isin(feature_values[extreme_mask])]
                    extreme_indices.update(extreme_songs.index.tolist())
                    
                    print(f"   {feature}: {len(extreme_songs)} canciones en extremos (p10={p10:.3f}, p90={p90:.3f})")
        
        # Combinar extremos con muestra aleatoria
        if extreme_indices:
            extreme_subset = cluster_data.loc[list(extreme_indices)]
            n_extreme = min(len(extreme_subset), target_size // 2)  # MÃ¡ximo 50% extremos
            
            if n_extreme > 0:
                selected_extreme = extreme_subset.sample(n=n_extreme, random_state=42)
                remaining_needed = target_size - n_extreme
                
                if remaining_needed > 0:
                    # Completar con muestra aleatoria del resto
                    remaining_data = cluster_data.drop(selected_extreme.index)
                    
                    if len(remaining_data) >= remaining_needed:
                        selected_random = remaining_data.sample(n=remaining_needed, random_state=42)
                        final_selection = pd.concat([selected_extreme, selected_random])
                    else:
                        # No hay suficientes datos restantes, usar todo
                        final_selection = pd.concat([selected_extreme, remaining_data])
                else:
                    final_selection = selected_extreme
            else:
                # Sin extremos vÃ¡lidos, fallback completo
                final_selection = cluster_data.sample(n=target_size, random_state=42)
        else:
            # Sin caracterÃ­sticas extremas identificadas, muestreo aleatorio
            print(f"âš ï¸  Sin extremos identificados, muestreo aleatorio")
            final_selection = cluster_data.sample(n=target_size, random_state=42)
        
        print(f"âœ… Fallback diversidad: {len(final_selection)} canciones (extremos: {len(extreme_indices) if extreme_indices else 0})")
        
        return final_selection
    
    def select_optimal_10k_with_validation(self, df):
        """
        SelecciÃ³n optimizada con validaciÃ³n Hopkins integrada.
        
        Implementa todas las mejoras crÃ­ticas identificadas:
        1. MaxMin sampling cientÃ­fico con selecciÃ³n inicial optimizada
        2. ValidaciÃ³n Hopkins continua durante selecciÃ³n  
        3. EliminaciÃ³n de normalizaciÃ³n doble mediante reutilizaciÃ³n de datos
        4. Estrategias de fallback cuando Hopkins se degrada
        
        Returns:
            tuple: (selected_df, selection_metadata)
        """
        print("\nğŸš€ SELECCIÃ“N OPTIMIZADA CON VALIDACIÃ“N HOPKINS")
        print("=" * 60)
        print("Mejoras: SelecciÃ³n cientÃ­fica, validaciÃ³n continua, sin doble normalizaciÃ³n")
        
        # Reset validador para nueva sesiÃ³n
        self.hopkins_validator.reset_validation_history()
        
        # Inicializar caracterÃ­sticas disponibles si no estÃ¡n definidas
        if not hasattr(self, 'available_features'):
            available_features = [f for f in self.musical_features if f in df.columns]
            self.available_features = available_features
            print(f"ğŸµ Inicializando caracterÃ­sticas: {len(available_features)}/{len(self.musical_features)} disponibles")
        
        # 1. Preparar datos para clustering
        print("\nğŸ”§ 1. PREPARANDO DATOS PARA CLUSTERING")
        print("-" * 40)
        
        X_scaled, clean_indices = self.prepare_clustering_data(df)
        if X_scaled is None:
            return None, None
            
        print(f"âœ… Datos preparados: {X_scaled.shape[0]:,} canciones Ã— {X_scaled.shape[1]} caracterÃ­sticas")
        
        # 2. Identificar estructura natural
        print("\nğŸ¯ 2. IDENTIFICANDO ESTRUCTURA NATURAL")
        print("-" * 40)
        
        cluster_labels, silhouette = self.identify_natural_clusters(X_scaled, clean_indices)
        
        # 3. Preparar DataFrame con clusters
        df_clean = df.loc[clean_indices].copy()
        df_clean['natural_cluster'] = cluster_labels
        
        # Crear mapeo para datos normalizados
        index_mapping = {original_idx: scaled_idx for scaled_idx, original_idx in enumerate(clean_indices)}
        
        print(f"\nğŸ“Š 3. SELECCIÃ“N POR CLUSTERS CON VALIDACIÃ“N")
        print("-" * 50)
        
        # 4. SelecciÃ³n por cluster con validaciÃ³n continua
        target_total = 10000
        selected_parts = []
        selection_log = {}
        
        for cluster_id in sorted(df_clean['natural_cluster'].unique()):
            cluster_data = df_clean[df_clean['natural_cluster'] == cluster_id]
            cluster_size = len(cluster_data)
            
            # Calcular selecciÃ³n proporcional robusta
            proportion = cluster_size / len(df_clean)
            base_target = int(target_total * proportion)
            
            # MEJORA: SelecciÃ³n proporcional robusta (evitar clusters vacÃ­os)
            min_per_cluster = max(10, target_total // 100)  # MÃ­nimo 1% o 10 canciones
            max_per_cluster = target_total // 2             # MÃ¡ximo 50%
            target_size = max(min_per_cluster, min(max_per_cluster, base_target))
            
            print(f"\nğŸµ Cluster {cluster_id}:")
            print(f"   TamaÃ±o original: {cluster_size:,} canciones ({proportion:.1%})")
            print(f"   Target base: {base_target:,}, ajustado: {target_size:,}")
            
            # Obtener Ã­ndices correspondientes en X_scaled
            cluster_scaled_indices = [index_mapping[idx] for idx in cluster_data.index if idx in index_mapping]
            
            # 5. MEJORA: Muestreo diverso mejorado
            try:
                selected_cluster = self.diverse_sampling_within_cluster_improved(
                    cluster_data, target_size, X_scaled, cluster_scaled_indices
                )
                
                # 6. NUEVO: ValidaciÃ³n Hopkins del cluster seleccionado
                if len(selected_cluster) >= 20:  # Suficientes datos para Hopkins
                    cluster_features = selected_cluster[self.available_features].dropna()
                    
                    if len(cluster_features) >= 10:
                        validation_result = self.hopkins_validator.validate_during_selection(
                            cluster_features, iteration=f"cluster_{cluster_id}", 
                            context=f"Cluster {cluster_id} con {len(selected_cluster)} canciones"
                        )
                        
                        print(f"   ğŸ“Š Hopkins validation: {validation_result['hopkins']:.3f}")
                        print(f"   ğŸ’¡ {validation_result['recommendation']}")
                        
                        # Guardar validaciÃ³n en metadata
                        self.selection_metadata['hopkins_validations'].append({
                            'cluster_id': cluster_id,
                            'hopkins': validation_result['hopkins'],
                            'action': validation_result['action'],
                            'size': len(selected_cluster)
                        })
                        
                        # Si Hopkins muy bajo, aplicar estrategia fallback
                        if validation_result['action'] == 'fallback':
                            print(f"   ğŸ”„ Hopkins bajo - aplicando fallback diversidad")
                            self.selection_metadata['fallback_activations'] += 1
                            
                            selected_cluster = self.apply_diversity_fallback(
                                cluster_data, target_size
                            )
                            
                            # Re-validar despuÃ©s del fallback
                            fallback_features = selected_cluster[self.available_features].dropna()
                            if len(fallback_features) >= 10:
                                fallback_validation = self.hopkins_validator.validate_during_selection(
                                    fallback_features, iteration=f"cluster_{cluster_id}_fallback"
                                )
                                print(f"   ğŸ”„ Hopkins post-fallback: {fallback_validation['hopkins']:.3f}")
                
                selection_log[cluster_id] = {
                    'original_size': cluster_size,
                    'selected_size': len(selected_cluster),
                    'selection_ratio': len(selected_cluster) / cluster_size,
                    'target_size': target_size,
                    'method': 'improved_maxmin_with_validation'
                }
                
                selected_parts.append(selected_cluster)
                print(f"   âœ… Seleccionadas: {len(selected_cluster):,} canciones")
                
            except Exception as e:
                print(f"   âŒ Error en cluster {cluster_id}: {e}")
                # Fallback a muestreo aleatorio en caso de error
                selected_cluster = cluster_data.sample(n=min(target_size, len(cluster_data)), random_state=42)
                selected_parts.append(selected_cluster)
                selection_log[cluster_id] = {
                    'original_size': cluster_size,
                    'selected_size': len(selected_cluster),
                    'error': str(e),
                    'method': 'random_fallback'
                }
        
        # 7. Combinar y ajustar tamaÃ±o final
        print(f"\nğŸ”§ 4. COMBINANDO Y AJUSTANDO SELECCIÃ“N FINAL")
        print("-" * 40)
        
        final_selection = pd.concat(selected_parts, ignore_index=True)
        initial_size = len(final_selection)
        
        print(f"TamaÃ±o inicial combinado: {initial_size:,}")
        
        if len(final_selection) != target_total:
            if len(final_selection) > target_total:
                print(f"ğŸ”§ Reduciendo de {len(final_selection):,} a {target_total:,}")
                final_selection = final_selection.sample(n=target_total, random_state=42)
            else:
                print(f"ğŸ”§ Completando de {len(final_selection):,} a {target_total:,}")
                remaining = target_total - len(final_selection)
                available = df_clean[~df_clean.index.isin(final_selection.index)]
                
                if len(available) >= remaining:
                    additional = available.sample(n=remaining, random_state=42)
                    final_selection = pd.concat([final_selection, additional], ignore_index=True)
                else:
                    print(f"âš ï¸  Solo {len(available)} canciones disponibles para completar")
                    final_selection = pd.concat([final_selection, available], ignore_index=True)
        
        # 8. NUEVO: ValidaciÃ³n Hopkins final del dataset completo
        print(f"\nğŸ” 5. VALIDACIÃ“N HOPKINS FINAL")
        print("-" * 40)
        
        final_features = final_selection[self.available_features].dropna()
        if len(final_features) >= 50:
            final_validation = self.hopkins_validator.validate_during_selection(
                final_features, iteration='FINAL_DATASET', 
                context=f'Dataset final con {len(final_selection)} canciones'
            )
            
            print(f"ğŸ“Š Hopkins final: {final_validation['hopkins']:.4f}")
            print(f"ğŸ’¡ {final_validation['recommendation']}")
        else:
            final_validation = {'hopkins': None, 'recommendation': 'Datos insuficientes para validaciÃ³n final'}
            print(f"âš ï¸  Datos insuficientes para validaciÃ³n Hopkins final")
        
        # 9. Generar metadata comprehensiva
        validation_summary = self.hopkins_validator.get_validation_summary()
        
        selection_metadata = {
            'selection_log': selection_log,
            'hopkins_validation': validation_summary,
            'final_hopkins': final_validation.get('hopkins'),
            'pre_clustering_silhouette': silhouette,
            'fallback_activations': self.selection_metadata['fallback_activations'],
            'diversity_fallbacks': self.selection_metadata['diversity_fallbacks'],
            'method_improvements': [
                'scientific_initial_selection',
                'continuous_hopkins_validation', 
                'elimination_double_normalization',
                'robust_proportional_selection',
                'diversity_fallback_strategy'
            ]
        }
        
        # 10. Resumen final
        print(f"\nâœ… SELECCIÃ“N CON VALIDACIÃ“N COMPLETADA")
        print("=" * 60)
        print(f"ğŸ“¦ Dataset final: {len(final_selection):,} canciones")
        print(f"ğŸ“Š Hopkins final: {final_validation.get('hopkins', 'N/A')}")
        print(f"ğŸ“ˆ Hopkins validaciones: {validation_summary['total_validations']}")
        print(f"ğŸ“Š Hopkins promedio: {validation_summary.get('hopkins_statistics', {}).get('avg_hopkins', 'N/A')}")
        print(f"ğŸ”„ Fallbacks activados: {self.selection_metadata['fallback_activations']}")
        print(f"ğŸ¯ Calidad estimada: {validation_summary.get('quality_assessment', {}).get('overall_quality', 'unknown')}")
        
        return final_selection, selection_metadata
    
    def validate_selection(self, selected_df, original_df):
        """Validar calidad de la selecciÃ³n."""
        print(f"\nğŸ” VALIDACIÃ“N DE LA SELECCIÃ“N")
        print("-" * 50)
        
        # Comparar distribuciones de top caracterÃ­sticas
        print("ğŸ“Š ComparaciÃ³n de distribuciones (original vs seleccionada):")
        
        for feature in self.top_features[:3]:  # Top 3 para no saturar output
            if feature not in selected_df.columns:
                continue
                
            orig_mean = original_df[feature].mean()
            sel_mean = selected_df[feature].mean()
            orig_std = original_df[feature].std()
            sel_std = selected_df[feature].std()
            
            print(f"   {feature}:")
            print(f"     Original: Î¼={orig_mean:.3f}, Ïƒ={orig_std:.3f}")
            print(f"     Seleccionado: Î¼={sel_mean:.3f}, Ïƒ={sel_std:.3f}")
            print(f"     Diferencia: Î”Î¼={abs(orig_mean-sel_mean):.3f}, Î”Ïƒ={abs(orig_std-sel_std):.3f}")
        
        # Verificar diversidad de gÃ©neros
        if 'playlist_genre' in selected_df.columns:
            orig_genres = set(original_df['playlist_genre'].unique())
            sel_genres = set(selected_df['playlist_genre'].unique())
            
            print(f"\nğŸµ Diversidad de gÃ©neros:")
            print(f"   Original: {len(orig_genres)} gÃ©neros Ãºnicos")
            print(f"   Seleccionado: {len(sel_genres)} gÃ©neros Ãºnicos")
            print(f"   ConservaciÃ³n: {len(sel_genres)/len(orig_genres)*100:.1f}%")
        
        print(f"\nâœ… ValidaciÃ³n completada")
    
    def save_dataset(self, selected_df):
        """Guardar dataset seleccionado."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Crear directorio de salida
        output_dir = "data/final_data"
        os.makedirs(output_dir, exist_ok=True)
        
        # Guardar CSV principal
        output_file = f"{output_dir}/picked_data_optimal_{timestamp}.csv"
        selected_df.to_csv(output_file, sep='^', decimal='.', index=False, encoding='utf-8')
        
        # TambiÃ©n crear versiÃ³n con nombre estÃ¡ndar
        standard_file = f"{output_dir}/picked_data_optimal.csv"
        selected_df.to_csv(standard_file, sep='^', decimal='.', index=False, encoding='utf-8')
        
        print(f"\nğŸ’¾ DATASETS GUARDADOS:")
        print(f"ğŸ“ Con timestamp: {output_file}")
        print(f"ğŸ“ VersiÃ³n estÃ¡ndar: {standard_file}")
        
        # Guardar metadatos
        metadata = {
            'timestamp': timestamp,
            'source_dataset': 'data/with_lyrics/spotify_songs_fixed.csv',
            'source_size': 18454,
            'selected_size': len(selected_df),
            'selection_method': 'clustering_aware_optimal',
            'features_used': self.available_features,
            'top_features_prioritized': self.top_features,
            'format': {
                'separator': '^',
                'decimal': '.',
                'encoding': 'utf-8'
            }
        }
        
        metadata_file = f"{output_dir}/picked_data_optimal_metadata_{timestamp}.json"
        import json
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ Metadatos: {metadata_file}")
        
        return standard_file

def main():
    """FunciÃ³n principal del selector optimizado con validaciÃ³n Hopkins."""
    print("ğŸ¯ SELECTOR OPTIMIZADO DE 10K CANCIONES DESDE 18K")
    print("=" * 60)
    print("Estrategia: Clustering-aware con validaciÃ³n Hopkins continua")
    print("Mejoras: SelecciÃ³n cientÃ­fica, sin doble normalizaciÃ³n, fallback inteligente")
    print("Basado en: AnÃ¡lisis clustering readiness (Hopkins=0.823)")
    print()
    
    # Inicializar selector con threshold Hopkins configurable
    selector = OptimalSelector(hopkins_threshold=0.70)
    
    # 1. Cargar dataset fuente
    print("ğŸ“‚ 1. CARGANDO DATASET FUENTE")
    print("-" * 30)
    
    df_18k = selector.load_source_dataset()
    if df_18k is None:
        print("âŒ ERROR: No se pudo cargar el dataset fuente")
        return
    
    # 2. SelecciÃ³n optimizada con validaciÃ³n
    print("\nğŸš€ 2. EJECUTANDO SELECCIÃ“N OPTIMIZADA")
    print("-" * 40)
    
    try:
        selected_10k, selection_metadata = selector.select_optimal_10k_with_validation(df_18k)
        
        if selected_10k is None:
            print("âŒ ERROR: FallÃ³ la selecciÃ³n optimizada")
            return
            
        print(f"\nâœ… SelecciÃ³n exitosa: {len(selected_10k):,} canciones")
        
    except Exception as e:
        print(f"âŒ ERROR en selecciÃ³n optimizada: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. ValidaciÃ³n tradicional adicional
    print("\nğŸ“Š 3. VALIDACIÃ“N TRADICIONAL ADICIONAL")
    print("-" * 40)
    
    try:
        selector.validate_selection(selected_10k, df_18k)
    except Exception as e:
        print(f"âš ï¸  Warning en validaciÃ³n tradicional: {e}")
    
    # 4. Guardar resultado con metadatos mejorados
    print("\nğŸ’¾ 4. GUARDANDO DATASET OPTIMIZADO")
    print("-" * 40)
    
    try:
        # Incluir metadatos de selecciÃ³n en el dataset guardado
        output_file = selector.save_dataset_with_metadata(selected_10k, selection_metadata)
        print(f"âœ… Dataset guardado: {output_file}")
        
    except AttributeError:
        # Fallback a mÃ©todo original si save_dataset_with_metadata no existe
        output_file = selector.save_dataset(selected_10k)
        print(f"âœ… Dataset guardado (mÃ©todo estÃ¡ndar): {output_file}")
        
        # Guardar metadatos por separado
        try:
            import json
            from datetime import datetime
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            metadata_file = f"data/final_data/selection_metadata_{timestamp}.json"
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(selection_metadata, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Metadatos guardados: {metadata_file}")
            
        except Exception as e:
            print(f"âš ï¸  No se pudieron guardar metadatos: {e}")
    
    # 5. Resumen final mejorado
    print(f"\nğŸ‰ SELECCIÃ“N OPTIMIZADA COMPLETADA")
    print("=" * 60)
    
    # InformaciÃ³n bÃ¡sica
    print(f"ğŸ“Š Dataset final: {len(selected_10k):,} canciones")
    print(f"ğŸ“ Archivo principal: {output_file}")
    print(f"ğŸ”„ Formato: Separador '^', decimal '.', UTF-8")
    
    # MÃ©tricas Hopkins
    final_hopkins = selection_metadata.get('final_hopkins', 'N/A')
    hopkins_validations = selection_metadata.get('hopkins_validation', {}).get('total_validations', 0)
    avg_hopkins = selection_metadata.get('hopkins_validation', {}).get('hopkins_statistics', {}).get('avg_hopkins', 'N/A')
    quality = selection_metadata.get('hopkins_validation', {}).get('quality_assessment', {}).get('overall_quality', 'unknown')
    
    print(f"\nğŸ“ˆ MÃ‰TRICAS DE CALIDAD:")
    print(f"   â€¢ Hopkins final: {final_hopkins}")
    print(f"   â€¢ Hopkins promedio: {avg_hopkins}")
    print(f"   â€¢ Validaciones realizadas: {hopkins_validations}")
    print(f"   â€¢ Calidad estimada: {quality}")
    print(f"   â€¢ Fallbacks activados: {selection_metadata.get('fallback_activations', 0)}")
    
    # InterpretaciÃ³n de calidad
    if isinstance(final_hopkins, (int, float)):
        if final_hopkins >= 0.80:
            status_emoji = "ğŸŸ¢"
            status_text = "EXCELENTE"
        elif final_hopkins >= 0.70:
            status_emoji = "ğŸŸ¡"  
            status_text = "BUENO"
        elif final_hopkins >= 0.60:
            status_emoji = "ğŸŸ "
            status_text = "ACEPTABLE"
        else:
            status_emoji = "ğŸ”´"
            status_text = "NECESITA MEJORAS"
    else:
        status_emoji = "âšª"
        status_text = "NO EVALUADO"
    
    print(f"\n{status_emoji} ESTADO FINAL: {status_text}")
    
    # Recomendaciones
    print(f"\nğŸ¯ MÃ‰TRICAS ESPERADAS PARA CLUSTERING:")
    expected_clustering_readiness = "75-85/100" if isinstance(final_hopkins, (int, float)) and final_hopkins >= 0.70 else "60-75/100"
    expected_silhouette = "0.15-0.20" if isinstance(final_hopkins, (int, float)) and final_hopkins >= 0.70 else "0.10-0.15"
    
    print(f"   â€¢ Clustering Readiness esperado: {expected_clustering_readiness}")
    print(f"   â€¢ Silhouette Score esperado: {expected_silhouette}")
    
    print(f"\nğŸ“‹ PRÃ“XIMOS PASOS:")
    if status_text in ["EXCELENTE", "BUENO"]:
        print("   âœ… Proceder a FASE 2: Clustering Comparativo")
        print("   ğŸ“ Ejecutar: clustering/algorithms/musical/clustering_optimized.py")
    elif status_text == "ACEPTABLE":
        print("   âš ï¸  Considerar mejoras opcionales antes de FASE 2")
        print("   ğŸ”§ O proceder con monitoreo adicional en clustering")
    else:
        print("   âŒ Revisar selecciÃ³n antes de continuar")
        print("   ğŸ”§ Considerar ajustar hopkins_threshold o datos fuente")
    
    print(f"\nğŸ“‹ ARCHIVOS GENERADOS:")
    print(f"   â€¢ Dataset: {output_file}")
    if 'metadata_file' in locals():
        print(f"   â€¢ Metadatos: {metadata_file}")
    
    print(f"\nğŸ”„ Para cargar el dataset:")
    print(f"   import pandas as pd")
    print(f"   df = pd.read_csv('{output_file}', sep='^', decimal='.', encoding='utf-8')")


def save_dataset_with_metadata(self, selected_df, selection_metadata):
    """Guardar dataset con metadatos integrados (mÃ©todo mejorado)."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Crear directorio de salida
    output_dir = "data/final_data"
    os.makedirs(output_dir, exist_ok=True)
    
    # Guardar CSV principal
    output_file = f"{output_dir}/picked_data_optimal_{timestamp}.csv"
    selected_df.to_csv(output_file, sep='^', decimal='.', index=False, encoding='utf-8')
    
    # TambiÃ©n crear versiÃ³n con nombre estÃ¡ndar
    standard_file = f"{output_dir}/picked_data_optimal.csv"
    selected_df.to_csv(standard_file, sep='^', decimal='.', index=False, encoding='utf-8')
    
    # Guardar metadatos integrados
    import json
    
    complete_metadata = {
        'generation_info': {
            'timestamp': timestamp,
            'generation_date': datetime.now().isoformat(),
            'source_dataset': 'data/with_lyrics/spotify_songs_fixed.csv',
            'selection_method': 'clustering_aware_with_hopkins_validation',
            'improvements_applied': selection_metadata.get('method_improvements', [])
        },
        'dataset_info': {
            'selected_size': len(selected_df),
            'features_count': len(self.available_features),
            'format': {'separator': '^', 'decimal': '.', 'encoding': 'utf-8'}
        },
        'selection_metadata': selection_metadata,
        'quality_summary': {
            'final_hopkins': selection_metadata.get('final_hopkins'),
            'avg_hopkins': selection_metadata.get('hopkins_validation', {}).get('hopkins_statistics', {}).get('avg_hopkins'),
            'quality_level': selection_metadata.get('hopkins_validation', {}).get('quality_assessment', {}).get('overall_quality'),
            'fallback_activations': selection_metadata.get('fallback_activations', 0)
        }
    }
    
    metadata_file = f"{output_dir}/picked_data_optimal_metadata_{timestamp}.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(complete_metadata, f, indent=2, ensure_ascii=False)
    
    # TambiÃ©n versiÃ³n estÃ¡ndar de metadatos
    standard_metadata = f"{output_dir}/picked_data_optimal_metadata.json"
    with open(standard_metadata, 'w', encoding='utf-8') as f:
        json.dump(complete_metadata, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ARCHIVOS GENERADOS:")
    print(f"ğŸ“ CSV principal: {standard_file}")
    print(f"ğŸ“ CSV con timestamp: {output_file}")
    print(f"ğŸ“‹ Metadatos: {metadata_file}")
    
    return standard_file

# Monkey patch para aÃ±adir mÃ©todo mejorado a la clase
OptimalSelector.save_dataset_with_metadata = save_dataset_with_metadata

if __name__ == "__main__":
    main()