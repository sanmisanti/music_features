#!/usr/bin/env python3
"""
AnÃ¡lisis Exploratorio Completo del Dataset de Canciones con Letras

Este script ejecuta el anÃ¡lisis exploratorio completo del dataset picked_data_lyrics.csv
que contiene 9,987 canciones con letras y caracterÃ­sticas musicales de Spotify.

Uso:
    python exploratory_analysis/run_full_analysis.py
    
Salidas:
    - Reportes comprensivos en outputs/reports/
    - Visualizaciones en formato PNG
    - AnÃ¡lisis estadÃ­stico detallado
    - Recomendaciones para clustering
"""

import sys
import time
from datetime import datetime
from pathlib import Path

# Agregar el directorio raÃ­z al path para importaciones
sys.path.append(str(Path(__file__).parent.parent))

from exploratory_analysis.reporting.report_generator import ReportGenerator
from exploratory_analysis.data_loading.data_loader import DataLoader
from exploratory_analysis.statistical_analysis.descriptive_stats import DescriptiveStats
from exploratory_analysis.visualization.distribution_plots import DistributionPlotter
from exploratory_analysis.visualization.correlation_heatmaps import CorrelationPlotter
from exploratory_analysis.feature_analysis.dimensionality_reduction import DimensionalityReducer

def print_header():
    """Imprime el encabezado del anÃ¡lisis"""
    print("ğŸµ" + "="*78 + "ğŸµ")
    print("           ANÃLISIS EXPLORATORIO COMPLETO - DATASET MUSICAL")
    print("ğŸµ" + "="*78 + "ğŸµ")
    print(f"ğŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ“ Dataset: data/final_data/picked_data_lyrics.csv")
    print("ğŸ¯ Objetivo: AnÃ¡lisis exploratorio para clustering musical")
    print("-" * 80)

def load_and_validate_data():
    """Carga y valida el dataset completo"""
    print("\nğŸ“Š PASO 1: CARGA Y VALIDACIÃ“N DE DATOS")
    print("-" * 50)
    
    loader = DataLoader()
    print("ğŸ”„ Cargando dataset completo picked_data_lyrics.csv...")
    
    start_time = time.time()
    result = loader.load_dataset('lyrics_dataset', sample_size=None, validate=True)
    load_time = time.time() - start_time
    
    if not result.success:
        print("âŒ Error al cargar el dataset:")
        for error in result.errors:
            print(f"   - {error}")
        return None, None
    
    data = result.data
    print(f"âœ… Dataset cargado exitosamente en {load_time:.2f} segundos")
    print(f"ğŸ“ Dimensiones: {data.shape[0]:,} filas Ã— {data.shape[1]} columnas")
    print(f"ğŸ’¾ Memoria utilizada: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Mostrar informaciÃ³n bÃ¡sica del dataset
    print(f"\nğŸ“‹ InformaciÃ³n del Dataset:")
    print(f"   â€¢ Canciones totales: {len(data):,}")
    print(f"   â€¢ CaracterÃ­sticas: {len(data.columns)}")
    print(f"   â€¢ Datos faltantes: {data.isnull().sum().sum():,}")
    print(f"   â€¢ Duplicados: {data.duplicated().sum():,}")
    
    # Mostrar caracterÃ­sticas musicales disponibles
    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()
    print(f"   â€¢ CaracterÃ­sticas numÃ©ricas: {len(numeric_cols)}")
    print(f"     {', '.join(numeric_cols[:10])}{'...' if len(numeric_cols) > 10 else ''}")
    
    return data, load_time

def perform_statistical_analysis(data):
    """Ejecuta anÃ¡lisis estadÃ­stico completo"""
    print("\nğŸ“ˆ PASO 2: ANÃLISIS ESTADÃSTICO DESCRIPTIVO")
    print("-" * 50)
    
    start_time = time.time()
    stats_analyzer = DescriptiveStats()
    
    print("ğŸ”„ Calculando estadÃ­sticas descriptivas...")
    stats_results = stats_analyzer.analyze_dataset(data)
    analysis_time = time.time() - start_time
    
    print(f"âœ… AnÃ¡lisis estadÃ­stico completado en {analysis_time:.2f} segundos")
    
    # Mostrar resumen de estadÃ­sticas
    if 'feature_stats' in stats_results:
        feature_stats = stats_results['feature_stats']
        print(f"ğŸ“Š EstadÃ­sticas calculadas para {len(feature_stats)} caracterÃ­sticas")
        
        # Mostrar algunas estadÃ­sticas destacadas
        print("\nğŸ¯ CaracterÃ­sticas Musicales Destacadas:")
        musical_features = ['danceability', 'energy', 'valence', 'speechiness', 'acousticness']
        for feature in musical_features:
            if feature in feature_stats:
                stats_obj = feature_stats[feature]
                # FeatureStats es una dataclass, acceder a atributos directamente
                mean_val = getattr(stats_obj, 'mean', 0)
                std_val = getattr(stats_obj, 'std', 0)
                print(f"   â€¢ {feature.capitalize()}: Î¼={mean_val:.3f}, Ïƒ={std_val:.3f}")
    
    # InformaciÃ³n de correlaciones
    if 'correlation_preview' in stats_results:
        corr_info = stats_results['correlation_preview']
        # Manejar tanto dict como objeto
        if hasattr(corr_info, 'high_correlations'):
            high_corr = len(corr_info.high_correlations)
        else:
            high_corr = len(corr_info.get('high_correlations', []))
        print(f"\nğŸ”— Correlaciones altas detectadas: {high_corr}")
    
    return stats_results, analysis_time

def generate_visualizations(data):
    """Genera visualizaciones del dataset"""
    print("\nğŸ¨ PASO 3: GENERACIÃ“N DE VISUALIZACIONES")
    print("-" * 50)
    
    start_time = time.time()
    
    # Crear directorio de salida
    output_dir = Path("outputs/reports/visualizations")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ”„ Generando distribuciones de caracterÃ­sticas...")
    
    # Generar distribuciones
    dist_plotter = DistributionPlotter()
    musical_features = ['danceability', 'energy', 'valence', 'speechiness', 'acousticness',
                       'instrumentalness', 'liveness', 'tempo', 'loudness']
    
    available_features = [f for f in musical_features if f in data.columns]
    
    if available_features:
        dist_results = dist_plotter.plot_feature_distributions(
            data, 
            features=available_features[:6],  # Primeras 6 caracterÃ­sticas
            plot_types=['histogram', 'boxplot']
        )
        print(f"âœ… Distribuciones generadas para {len(available_features)} caracterÃ­sticas")
    
    # Generar mapas de calor de correlaciÃ³n
    print("ğŸ”„ Generando mapas de calor de correlaciÃ³n...")
    corr_plotter = CorrelationPlotter()
    
    numeric_data = data.select_dtypes(include=['float64', 'int64'])
    if len(numeric_data.columns) >= 2:
        corr_fig = corr_plotter.create_correlation_heatmap(numeric_data)
        if corr_fig:
            corr_path = output_dir / "correlation_heatmap.png"
            corr_fig.savefig(corr_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Mapa de calor guardado: {corr_path}")
            
            # Cerrar figura para liberar memoria
            import matplotlib.pyplot as plt
            plt.close(corr_fig)
    
    viz_time = time.time() - start_time
    print(f"âœ… Visualizaciones completadas en {viz_time:.2f} segundos")
    
    return viz_time

def perform_dimensionality_analysis(data):
    """Ejecuta anÃ¡lisis de dimensionalidad y PCA"""
    print("\nğŸ”¬ PASO 4: ANÃLISIS DE DIMENSIONALIDAD (PCA)")
    print("-" * 50)
    
    start_time = time.time()
    
    # Seleccionar solo caracterÃ­sticas numÃ©ricas
    numeric_data = data.select_dtypes(include=['float64', 'int64'])
    print(f"ğŸ”„ Analizando {len(numeric_data.columns)} caracterÃ­sticas numÃ©ricas...")
    
    reducer = DimensionalityReducer()
    
    # AnÃ¡lisis PCA
    pca_results = reducer.fit_pca(numeric_data, variance_threshold=0.90)
    
    if pca_results:
        n_components = pca_results.get('n_components', 0)
        variance_explained = pca_results.get('total_variance_explained', 0)
        
        print(f"âœ… PCA completado:")
        print(f"   â€¢ Componentes requeridos: {n_components}")
        print(f"   â€¢ Varianza explicada: {variance_explained:.1%}")
        
        # Mostrar los primeros componentes
        if 'component_analysis' in pca_results:
            comp_analysis = pca_results['component_analysis']
            print(f"\nğŸ¯ Primeros Componentes Principales:")
            for i, (comp_name, comp_info) in enumerate(list(comp_analysis.items())[:3]):
                # Manejar tanto dict como objeto
                if hasattr(comp_info, 'explained_variance_ratio'):
                    var_ratio = comp_info.explained_variance_ratio
                    interpretation = getattr(comp_info, 'interpretation', 'N/A')
                else:
                    var_ratio = comp_info.get('explained_variance_ratio', 0)
                    interpretation = comp_info.get('interpretation', 'N/A')
                print(f"   â€¢ {comp_name}: {var_ratio:.1%} - {interpretation}")
    else:
        print("âš ï¸  PCA no pudo completarse - posiblemente insuficientes caracterÃ­sticas numÃ©ricas")
    
    # t-SNE para una muestra (computacionalmente intensivo)
    sample_size = min(1000, len(numeric_data))
    if len(numeric_data) > sample_size:
        print(f"ğŸ”„ Ejecutando t-SNE en muestra de {sample_size:,} canciones...")
        sample_data = numeric_data.sample(n=sample_size, random_state=42)
        tsne_results = reducer.fit_tsne(sample_data, n_components=2)
        
        if tsne_results:
            # Manejar tanto dict como objeto
            if hasattr(tsne_results, 'kl_divergence'):
                kl_div = tsne_results.kl_divergence
            else:
                kl_div = tsne_results.get('kl_divergence', 0)
            print(f"âœ… t-SNE completado (KL divergence: {kl_div:.4f})")
    
    analysis_time = time.time() - start_time
    print(f"âœ… AnÃ¡lisis de dimensionalidad completado en {analysis_time:.2f} segundos")
    
    return pca_results, analysis_time

def generate_comprehensive_report(data, stats_results, pca_results, total_time):
    """Genera el reporte comprensivo final"""
    print("\nğŸ“‹ PASO 5: GENERACIÃ“N DE REPORTE COMPRENSIVO")
    print("-" * 50)
    
    start_time = time.time()
    
    # Usar el generador de reportes
    report_generator = ReportGenerator("outputs/reports/")
    
    print("ğŸ”„ Generando reporte comprensivo...")
    report_paths = report_generator.generate_comprehensive_report(
        dataset_type='lyrics_dataset',
        sample_size=None,  # Dataset completo
        include_visualizations=True,
        formats=['json', 'markdown', 'html']
    )
    
    report_time = time.time() - start_time
    
    print(f"âœ… Reportes generados en {report_time:.2f} segundos")
    print("\nğŸ“ Archivos de salida:")
    for format_type, path in report_paths.items():
        if path:
            print(f"   â€¢ {format_type.upper()}: {path}")
    
    return report_paths, report_time

def print_summary(data, total_time, load_time, analysis_time, viz_time, pca_time, report_time):
    """Imprime resumen final del anÃ¡lisis"""
    print("\nğŸ‰ ANÃLISIS EXPLORATORIO COMPLETADO")
    print("ğŸµ" + "="*78 + "ğŸµ")
    
    print(f"\nğŸ“Š RESUMEN DEL ANÃLISIS:")
    print(f"   â€¢ Dataset procesado: {len(data):,} canciones")
    print(f"   â€¢ CaracterÃ­sticas analizadas: {len(data.select_dtypes(include=['float64', 'int64']).columns)}")
    print(f"   â€¢ Tiempo total: {total_time:.2f} segundos")
    
    print(f"\nâ±ï¸  DESGLOSE DE TIEMPOS:")
    print(f"   â€¢ Carga de datos: {load_time:.2f}s")
    print(f"   â€¢ AnÃ¡lisis estadÃ­stico: {analysis_time:.2f}s")
    print(f"   â€¢ Visualizaciones: {viz_time:.2f}s") 
    print(f"   â€¢ AnÃ¡lisis PCA: {pca_time:.2f}s")
    print(f"   â€¢ GeneraciÃ³n reportes: {report_time:.2f}s")
    
    print(f"\nğŸš€ PRÃ“XIMOS PASOS RECOMENDADOS:")
    print("   1. Revisar los reportes generados en outputs/reports/")
    print("   2. Analizar las correlaciones entre caracterÃ­sticas")
    print("   3. Evaluar los resultados del PCA para clustering")
    print("   4. Proceder con el anÃ¡lisis de clustering K-means")
    
    print("\nâœ… El dataset estÃ¡ listo para clustering musical!")
    print("ğŸµ" + "="*78 + "ğŸµ")

def main():
    """FunciÃ³n principal que ejecuta todo el anÃ¡lisis"""
    overall_start = time.time()
    
    print_header()
    
    try:
        # Paso 1: Cargar datos
        data, load_time = load_and_validate_data()
        if data is None:
            print("âŒ No se pudo cargar el dataset. Abortando anÃ¡lisis.")
            return 1
        
        # Paso 2: AnÃ¡lisis estadÃ­stico
        stats_results, analysis_time = perform_statistical_analysis(data)
        
        # Paso 3: Visualizaciones
        viz_time = generate_visualizations(data)
        
        # Paso 4: AnÃ¡lisis de dimensionalidad
        pca_results, pca_time = perform_dimensionality_analysis(data)
        
        # Paso 5: Reporte final
        report_paths, report_time = generate_comprehensive_report(
            data, stats_results, pca_results, time.time() - overall_start
        )
        
        # Resumen final
        total_time = time.time() - overall_start
        print_summary(data, total_time, load_time, analysis_time, viz_time, pca_time, report_time)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ ERROR DURANTE EL ANÃLISIS:")
        print(f"   {str(e)}")
        print("\nğŸ”§ Verifica que:")
        print("   â€¢ El archivo picked_data_lyrics.csv existe")
        print("   â€¢ Las dependencias estÃ¡n instaladas")
        print("   â€¢ Hay suficiente memoria disponible")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)